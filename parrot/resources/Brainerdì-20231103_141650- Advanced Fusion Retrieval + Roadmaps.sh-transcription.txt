Ok ci siamo tutti. Allora ragazzi benvenuti in questo nuovo formato del Brainerd, oggi facciamo la prima puntata, ovviamente ce ne saranno delle altre e speriamo possiate collaborare tutti, anzi ne approfitto vi condivido lo schermo con la Bachecatrello dove abbiamo già messo iof ed emino, qualche spunto per le proposte future. Ovviamente tutti liberi di contribuire sia con quello che si mette sulla ml newsletter sia con argomenti magari a piacere, ad esempio qua ci sono molti argomenti di cui in realtà non abbiamo postato nessun articolo sulla newsletter però potenzialmente potete mettere tutte le proposte che volete. Per il prossimo venerdì chi vuole fare, chi vuole spiegare qualcosa sempre in accordo col messaggio che ha mandato Giorgione sul sul gruppo, che ha mandato però sull'altro gruppo sul vecchio gruppo non su quello della nuova chiamata quindi magari poi faccio copia e incolla, è libero di farlo quindi si accettano le proposte. Ora oggi parliamo come ho detto nel messaggio sul gruppo di principalmente due cose che sono state pubblicate da Vincenzo Asta e da Marco Ganassin sull'ml newsletter quindi di Advanced Fusion Retriever con la MyIndex con l'Unchain e poi facciamo vedere un secondo la roadmap di Prompt Engineering ma potenzialmente di qualsiasi altra tecnologia slash argomento, strumento interessante potrebbe essere utile. Allora partendo dal primo argomento Advanced Fusion Retriever a cosa serve e perché lo stiamo vedendo? Allora in azienda soprattutto sappiamo che spesso abbiamo a che fare con sistemi di RUG che significa Retrieval Augmented cioè sistemi in cui noi facciamo Retrieval attraverso un VectorDB, un VectorStore, un IndexStore eccetera prendiamo un set di documenti una base documentale la forniamo nel Prompt dell'LLM per dargli una specie di base di conoscenza e ovviare al problema tale per cui le ILM hanno una conoscenza molto vasta ma poco specifica su alcuni argomenti. Ora nel tempo cioè in questi ultimi mesi soprattutto anzi in quest'ultimo annetto questo argomento è molto caldo ci sono tantissime persone che lavorano su questi anche su progetti interni diciamo all'interno della nostra azienda su sistemi RUG penso al chatbot casa su cui lavoriamo io Giuseppe Maddalena e Andrea ci sono anche altri sicuramente non so se in Audi in realtà è basato su RUG oppure se è finzionato però è basato su RUG e allora anche questi progetti insomma è appunto un'alternativa più diciamo anche efficace forse al fine tuning sicuramente è meno onerosa sia in termini di costi sia in termini di tempo. Ora appunto per fare Retrieval noi solitamente utilizziamo un VectorStore un VectorDB quindi abbiamo degli indici con dei documenti facciamo gli embedding di questi documenti e poi attraverso una similarità una funzione similarità estraiamo il risultato il documento che interessa data l'acquiri dell'utente. Questo è il modo più semplice più immediato di farlo poi ci sono altri VectorStore ci sono i i GraphDB quindi utilizzando grafi di knowledge graph eccetera oggi vediamo un nuovo metodo che sta spopolando in queste ultime settimane che è basato su un pezzo di codice di questo carissimo signore che non so come nemmeno come si chiama e che si chiama Advanced Fusion Retrieval. Cosa fa l'Advanced Fusion Retrieval? Partiamo da quello base quindi quello su cui poi si è ispirato la MyIndex

sostanzialmente, per evitare che i documenti importanti per rispondere a una certa domanda utente non siano sufficienti, comunque che il sistema che noi stiamo utilizzando di Retrieve perda delle informazioni importanti, noi facciamo un passaggio prima con un LLM che ci permette di generare delle alternative di una certa query utente, quindi sostanzialmente riscrivere n volte una query dell'utente, e per ognuna di queste query fare la ricerca all'interno del vector store e poi applicare un algoritmo, che è il Reciprocal Rank Fusion, che vediamo tra poco, per fare il re-scoring, il re-rank di questi documenti, e poi prendere i documenti più rilevanti, tipo i primi k documenti tra questi, e generare la nostra risposta, quindi fornire il contesto all'LLM e fornire anche la risposta successiva. Basandosi su questo, la MyIndex ha implementato questo Query Fusion Retriever, che fa la seguente cosa, il primo passaggio è identico, cioè lui prende un LLM, una query dell'utente, non so se vedete il mio cursore, forse no, lo vedete, ok perfetto, prende la query dell'utente, attraverso un LLM con un prompt ad hoc del tipo, dato questa query, genera n query, che sono prese simili a questa query ma riformulate, generiamo n query. Ora, cosa fa l'Advanced Fusion Retriever di la MyIndex? A differenza dell'altro, utilizza un altro concetto che è quello dell'Ensemble Retriever, ovvero noi possiamo avere vari vector store che fanno la retrieve dei documenti sfruttando degli algoritmi differenti, uno può essere un algoritmo a keyword, per esempio, mentre un altro retriever da poter utilizzare, un altro metodo di retrieve, può essere quello basato sugli indici, quindi sulla similarità semantica, come usiamo spesso noi, e per ognuna di queste query facciamo la retrieve da uno o più vector store sfruttando un diverso algoritmo, i diversi algoritmi, quindi abbiamo n liste di n documenti che vengono restituiti, a quel punto noi possiamo applicare questo algoritmo di Reciprocal Rank Fusion, prendiamo i n documenti più rilevanti e poi facciamo la sintesi che sostanzialmente si tratta semplicemente di prendere il contesto, la query iniziale dell'utente e fornire una risposta, la risposta finale del nostro chatbot motore di summarization. In la MyIndex, prima faccio vedere qua come farlo from scratch, diciamo, come la risorsa che ha fornito Vincenzo, e poi faccio vedere il metodo più veloce e immediato, quindi qui cosa facciamo? Qui scegliamo il nostro LLM, il nostro modello di embedding, andiamo a settare tutto quello che serve per generare vector store e generare i documenti che ci servono, io qua ho già calcolato il vector store, quindi lo andiamo semplicemente a caricare dallo storage.

documenti che sono tipo un codice etico di una banca, DB per banca, insomma c'avevo sottomano l'usato. Quindi qua carichiamo il VectorStore, attraverso questo prompt generiamo n query, in particolare qui generiamo un numero di query che è uguale a numQueries che è un parametro che gli forniamo noi, in query va la query dell'utente e qua ci facciamo restituire un JSON dove c'è un campo queries che contiene la lista di nuove query. Ovviamente le query saranno le query generate più quelle originali dell'utente, quindi attraverso questo noi andiamo a generare delle query, questo è un esempio, quindi ad esempio due codici etici B per B vengono generate altre tre query, che sono codici etici B per banca, significato dei codici etici, B per banca, sostenibilità. Dopodichè cosa facciamo? Noi andiamo a fare la retrieve per ogni query e per ogni tipo di retriever che stiamo utilizzando dei documenti. Ora questo è un passaggio piuttosto importante anche a livello computazione, nel senso che noi qui stiamo parallelizzando molto, siamo a sinchio e tutto, però c'è da tenere in mente che con nQuery e mRetriever noi ci andiamo ad estrarre un certo numero di documenti, sicuramente più rilevante di quello che avremmo estratto utilizzando un solo retriever e una sola query, quindi anche questo in termini di tempo è considerevole se non si usa la parallelizzazione. Questo lo dico per, insomma, se ci sono dei limiti temporali entro i quali deve essere fatta la computazione, della risposta eccetera, queste sono cose su cui ragionare. Qui stiamo utilizzando, utilizziamo per questo esempio, il nostro vector store che fa la ricerca per similarità semantica, quindi abbastanza base, e poi utilizziamo un algoritmo che è il BM25, che è sostanzialmente un algoritmo messo qui in una piccola fonte Wikipedia, dove spiega cos'è l'algoritmo, sostanzialmente per farla breve, una specie di retriever che usa le keyword, è l'IDF, l'Inverse Document Frequency, quindi algoritmi un po' più classici, ecco l'Information Retriever. Dopodiché, dopo aver estratto i documenti, applichiamo l'algoritmo di re-ranking, quindi Reciprocal Rerank Fusion, che fa due cose sostanzialmente. Fa il re-ranking dei documenti attraverso il rango, che sarebbe l'ordine in cui sono stati, in cui si trovano, l'ordine è determinato dallo score di similarità, o comunque lo score che è restituito dal nostro retriever, quindi quanto è rilevante un certo documento, e viene calcolato attraverso questa formula. Quindi è la somma di tutte le volte che appare il documento.

La somma che è 1 su K che è il rango, scusate K che è una costante che serve a controllare il bilanciamento diciamo dell'importanza dei documenti che hanno un rank molto alto e bilanciare anche quelli che hanno un rank molto basso e poi il rango del nostro documento, del documento stesso. Questa è la funzione che implementa la MyIndex, in realtà è molto semplice e fa anche implicitamente un'altra cosa ovvero la deduplicazione dei documenti, quindi anche se un retriever prende lo stesso documento per tante volte tu te lo ritroverai solo una volta alla fine di tutto il giro. Dopodiché con questo parametro di similarità andiamo a prendere soltanto i top K documenti più rilevanti. Questo qua ho fatto già un esempio, quindi Fused Results, prendiamo i nostri risultati, prendiamo la top K similarità e questi sono i documenti che sono estratti. Ora come potete vedere in realtà implementativamente è un po' lungo da fare, per questo la MyIndex ci fornisce il Query Fusion Retriever che è un wrapper che fa tutte queste cose, quindi noi gli forniamo una lista di retriever che vogliamo utilizzare, una similarity top K che vogliamo utilizzare, quante query vogliamo generare, se vogliamo usare AsyncIO eccetera e poi possiamo pure dirgli la modalità con cui fare la Fusion dei documenti, questo perché esiste anche la modalità semplice dove semplicemente tu fai soltanto la deduplicazione dei documenti e ti basi sullo score che viene restituito dai vari retriever. Il Reciprocal Rank Fusion in realtà per ora sta spopolando molto perché è un algoritmo molto efficace in Information Retrieval per questi task. Qui facciamo girare la nostra chain tra virgolette end to end, quindi gli forniamo semplicemente la query che vogliamo dare al nostro modello, alla nostra chain, quindi quali sono i tre principi etici di B per Bank, lui internamente genererà nQuery, prenderà i documenti attraverso questi retriever e genererà la risposta finale che è questa qui, quindi i principi etici di B per Bank sono Integrità, Onestà e Correttenza, tutto molto bello. Se non usiamo l'Amaindex si può fare la stessa cosa con l'Unchain, un po' più verboso, non ci sono ancora delle classi di Wrapper che fanno proprio questa cosa qui. In Unchain esistono dei retriever che sono l'Ensemble Retriever, dove tu attraverso una query puoi interrogare più vector store o lo stesso vector store con più retriever diversi o alternativamente c'è il Multi Query Retriever che genera nQuery e fa le query attraverso un solo retriever. Non c'è ancora la fusione di questi, però si può implementare in modo abbastanza semplice, quindi qui ho fatto sostanzialmente la stessa cosa che facevo all'inizio di quell'altro notebook.

Quindi creiamo il datastore, il vector store e tutto. Qui generazione delle query, stesso identico prompt. Questa è la chain finale che ti genera la risposta. Qui creiamo i nostri due retriever, quindi attraverso l'ensemble retriever di LangChain diciamo di utilizzare questo BM25 retriever, il vector store con gli embedding e similarità semantica base. Questa è la funzione per calcolare il reciprocal rank, questa l'ho presa direttamente dall'implementazione dell'ensemble retriever che fa proprio questa fusione attraverso il reciprocal rank. Dopodiché questa funzione aggrega il tutto, quindi generiamo le query, attraverso il retriever prendiamo i documenti, applichiamo il reciprocal rank e poi forniamo la risposta. Quindi questo sostanzialmente è quello che fa. Può essere molto utile in alcuni progetti sicuramente, c'è da tenere in conto la complessità di avere più vector store o più retriever sullo stesso vector store, però può essere un metodo molto efficace per non perdere dei documenti che potrebbero essere importanti e che rimangono diciamo al di sotto dei primi N documenti considerati importanti utilizzando una sola query e un solo tipo di retriever. Quindi potrebbe essere un metodo interessante da utilizzare per prossimi progetti oppure per progetti anche in corso. Se non ci sono domandine passerei all'altra risorsa che ci ha fornito il nostro caro Marco Gana. La mia domanda è se tu nelle prove, non so se hai avuto tempo di fare tante prove, se realmente come dicono quelli di Lama Index ci sono dei miglioramenti sulla prestazione. C'è da dire una cosa, io l'ho provato su un vector store che è veramente una minchiata, cioè è un vector store che ha 50-40 documenti, 40 c'è anche in tutto, quindi non si può stimare in realtà su questo caso il funzionamento migliore di altre fonti. Nel senso per farti un esempio, ne abbiamo parlato di recente, questo risultato qui è praticamente lo stesso che mi viene dato da GNAP Builder dando gli impasti ai documenti così e via. Quindi in realtà non ho fatto tante prove, non ti so dire se effettivamente è efficace come dicono. Io credo che sicuramente il fatto di dare più query riformulate ti dia un contesto molto più ampio. Ci sono tecniche di retrieval che sono migliori di quella semplice e che non sono così complesse.

Una tecnica di cui parlavo con Andrea l'altra volta era quella di fare dei chunk molto piccoli su un documento e poi espandere il contesto, cioè la finestra del contesto, del documento che stai, da cui hai preso quel piccolo chunk in modo tale da espandere il documento. Anche questo secondo me è una tecnica che è molto efficace e non è sicuramente così complessa come questa qui. Comunque per rispondere alla tua domanda in realtà non ho fatto così tante prove da poter capire se effettivamente migliora di così tanto i risultati. Sì, però condivido su quell'idea che abbiamo avuto anche noi in modo da avere poi degli embedding un po' più specifici, no? Andare proprio sulle parole e frasi veramente vicine e poi espandere, sì, quella è pure una buona strada. Sicuramente magari questa approcciativa qui è da capire poi se provare un solo step o tutti insieme secondo se la soluzione deve rispondere veloce e quindi magari eliminare il primo step e partire dal secondo e via. Sono riequisito, grazie. Dedica a voi. Ok, se non ci sono altre domande facciamo vedere le robe. Che ce ne sono in realtà, sai, a partire da Luca che ha la mani alzata. Ah, raga, scusatemi, non vedo io le mani. Ci sono Luca, io e in realtà ha scritto prima anche Fede Minuturi in chat. Il caro, vediamo la chat, scusate. Sei molto richiesto. Quando parla di hack standard, ma no, non ci sono dei numeri ben precisi, secondo me puoi fare quello che vuoi. Cioè in termini di query, secondo me, se la query è molto semplice poi il modello finisce per generarti n query che sono tutte molto simili e quindi non hai un grande vantaggio. Per quanto riguarda il numero di retrievers, secondo me, in realtà è molto a discrezione. Però non ho trovato dei benchmark, diciamo, per punare, tra virgolette, questi due parametri. Il risposto è già dato? Eh, le must judge lo lascio come argomento per il prossimo. L'ho messo già fra i proposal e ne parleremo, magari il prossimo venerdì. Perché anche quello, secondo me, è un argomento abbastanza extensive. Non so se ho risposto alle tue domande, Fede, però in caso scrivi di nuovo in chat. Altri due. Vai, Luca, che lo vedo con la mano inalzata, numero uno. Ciao amici, innanzitutto grazie Giorlando. Volevo chiederti una cosa, vediamo se mi ricordo tutto. La prima è che abbiamo detto del tempo di risposta per quanto riguarda più retriever che possa andare in parallelo, però in Python non è proprio in parallelo perché lo sappiamo tutti. C'è un'altra cosa, però, da considerare che appena ti arriva la domanda tu devi fare anche la generazione delle n query che tu vuoi. Quindi c'è un'altra chiamata in mezzo che si interpone fra la domanda e la risposta. Quindi anche questo, secondo me, influisce sul tempo di risposta e non di pochissimo. Sono tre query, un paio di secondi secondo me ti parto.

per generarle tutte. Sì, sì, infatti è molto... nel senso che se te lo puoi permettere magari è un modo di procedere che può essere efficace, però bisogna sempre stare attenti, ad esempio su Unipol noi abbiamo il timeout limit che ora non mi ricordo quant'è, 10 secondi, 15 secondi, eccetera, che comunque ce la si fa secondo me, però dipende sempre da appunto quanti retriever usi, quanti query usi, perché se poi metti generami 20 query e usa 10 retrieval, là diventa un pochino forse complesso, soprattutto con retriever che sfruttano algoritmi un po' più classici che si devono magari generare il vocabolario, tutte ste robe così, vabbè in realtà quello lo carichi quindi non è un problema ora pensandoci, però non so, bisognerebbe testarlo, ecco farei dei benchmark come diceva Fede. Ok, grazie, ti chiedo un'altra cosa, in pratica l'out di questa cosa sono comunque n-nodi, a prescindere da quanti retriever hai utilizzato, tu praticamente stai usando un'altra chiamata generativa per splittarti la domanda in anche domande più semplici tecnicamente, quindi fai in modo di spaziare di più e di beccare dei nodi che potrebbero essere un pochino più coerenti per fornire la risposta, però alla fine tu dai più nodi, semplicemente, ma non fai nulla a livello di manipolazione del dato, cioè se i nodi sono brutti, quindi generalmente sono troppo grandi, oppure come abbiamo detto anche prima con Vincenzo, cioè se non hai nodi piccoli poi dici che magari ce ne siano troppi, eccetera, eccetera, però non fai nulla a livello di manipolazione qua, dai semplicemente più nodi. Sì, però facciamo pure, aspetta che lo riprendo, ecco la MyIndex ad esempio internamente fa questa cosa qui, quindi tu gli specifici, devi specificare un certo similarity top k e lui ti restituisce effettivamente tanti documenti, perché tu fai più query su più retriever, ogni retriever ti restituisce una lista di documenti che sarà lo standard, cioè scusa il default di documenti da restituire, poi c'è effettivamente la roba del nodo. Prendi i migliori dai migliori, diciamo, cioè prendi i migliori dai migliori, sì sì sì. Esatto, poi c'è la cosa di deduplicazione che un po' migliora questo aspetto, perché cioè se tu hai un nodo con lo stesso testo identico, lui capisce e fa proprio qui, aggiunge il rank, però per lui è lo stesso testo, è lo stesso nodo identico, quindi fa la deduplicazione e fa sta cosa del ranking. Questo migliora la cosa, in la MyIndex è un pochino peggio tutta la roba, perché bisogna farlo a manella questa cosa qui, però sì, nel senso lui ti restituisce un elenco sicuramente più ampio di documenti e poi devi occuparti tu di prendere i top k che a te interessano. Sera, questa è la domanda se l'ho capita bene? Sì sì, certo. Ok, perfetto.

Grazie Sabina, e dai Marcolone. Grazie innanzitutto, molto interessante tutto il megadiscorso, a me non è chiara una parte sui Retriever, cioè da quello che ho capito io, ciascuno dei Retriever va a calcolarsi il suo score, ma per ciascuno ha delle quivi, sia quella passata all'utente, sia quelle generate, ok? Quindi tendenzialmente avremo uno score per la quivi, uno fatto all'utente e N per quelle generate, ma c'è un peso su questi score per esempio per far prevalere la quivi dell'utente, valgono tutte uguali? Mi viene il sospetto che possano esserci dei problemi nel momento in cui vengono generate le quivi e qualcuno di questi sbarella e magari tira su dei documenti che magari non c'entrano niente, quella quivi originale dell'utente, non so se c'è un'anarchia su questo punto, se c'è qualche controllo o qualche evidenza che funziona. Allora, sicuramente una cosa che la Mindex ho visto che non fa, che però fa l'Unchain, perché se vedi ho preso appunto questa funzione dalla loro implementazione e se vedi c'è scritto weighted, ora io ho tolto i pesi effettivamente perché sono mezzi inutili, però tu almeno per il Retriever puoi utilizzare dei pesi, quindi se vuoi dare più peso ad esempio al Retriever che fa la ricerca per Keyword puoi farlo, oppure se vuoi dare più peso alla ricerca attraverso Embedding lo puoi fare. Per quanto riguarda le query invece non ho trovato dei pesi se devo essere sincero, però potrebbe essere di facile implementazione nel senso che tu sai che ogni query per ogni Retriever ti restituisce una certa lista di documenti, quindi se tu hai per n query n liste di documenti tu puoi farti una lista di n pesi e pesare magari gli score, quindi moltiplicarti gli score quando fai il ranking, quindi fra una cosa del genere. Ha senso come cosa in realtà, perché tu dici la query dell'utente si spera, spoiler, in realtà non è sempre così perché a volte gli utenti scrivono cose che in italiano non hanno nemmeno senso, però è ben formulato, comunque formulata meglio rispetto a query che possono essere generate e che sbarellano come nel caso che abbiamo visto prima dove chiedeva cose che non c'entravano tanto, tipo sostenibilità, che non c'entrano cazzo con due codici etici BIPER in realtà. Quindi non ho visto che si può fare questa cosa che dici tu, quindi applicare i pesi per le query, però teoricamente si può fare, cioè basta modificare la funzione di re-rank direi. Ottimo, volevo più che altro una tua valutazione sul fatto che fosse una preoccupazione legittima, perché a me sembra abbastanza lampante e in realtà mentre parlavi mi è venuto in mente un altro discorso, proprio poiché le query che poi determinano quel famoso ranking definitivo di tutti i documenti sono generate, ogni volta io mi aspetto siano leggermente diverse perché ci sarà un po' di temperatura, questo significa che la lista di documenti ritrovati ogni volta potrà essere un po' diversa, quindi non è determinissimo quel ranking, niente, è una piccola considerazione che magari qualcuno potrebbe far sorcere il naso, però that's all, ok, grazie mille, a te.

Ok controllo, scusate, la chat per vedere se ci sono. Ok va bene allora dai velocemente perché siamo, abbiamo già un po' sforato. Vediamo quello che ci ha fornito il grande e caro Marco Gana. Questo è un sito, roadmap.sh, anzi vi metto, scusate, vi metto il link qui in chat perché per chi non l'avesse visto su ML Newsletter. Allora questo è un sito molto interessante, ha questa interfaccia grafica molto carina, quindi come vedete è una roadmap con degli step. Ogni step, ogni macro step ha poi dei sottostep cliccabili che ti danno una specie di spiegazione su questo sottostep e anche dei link che possono essere utili. Quindi Introduction to LLM ci rimanda a questo bellissimo articolo. Ora, secondo me può essere utile soprattutto per il Prompt Engineering, questo che ha postato appunto Marco, perché stiamo cercando di creare magari delle risorse, una specie di documentazione per Prompt Engineering con degli esempi o comunque delle buone pratiche da seguire per vari use case. Quindi un chatbot oppure per fare summary session, per fare non so che cosa, insomma, o per fare calcoli matematici, non so, qualcosa del genere. E questa risorsa devo dire che è abbastanza interessante perché ti spiega i concetti base, ti spiega come, i vari stili comunque, le varie tecniche per fare prompting, quindi c'è few shot, chain of thought e tutto quanto, e ci sono anche vari pitfalls che possono sorgere nel fare prompting o comunque negli LLM, quindi qua vedo prompt hacking, prompt injection, prompt leaking, jailbreaking e tutto quanto, che sono cose a cui dobbiamo fare soprattutto per cose che vanno in produzione, per sistemi o per clienti che hanno dati sensibili, o semmai faremo degli LLM che possono accedere a dati che sono sensibili o degli utenti o del cliente, dobbiamo stare molto attenti a queste cose qui. Niente, questa è una risorsa interessante, vedevo che ci sono comunque varie roadmap per vari altri argomenti, quindi se un giorno vorrete ad esempio diventare dei C++ developer, qua c'è una bella roadmap dove vi spiega C++ dall'introduzione fino alle cose un po' più difficili, tipo gli smart pointers e tutto, quindi molto molto carino, grazie Marco di averla condivisa perché secondo me potrebbe essere molto interessante utilizzarla. Grazie a te. Se vuoi dire qualcosa, anzi se vuoi dire tu qualcosa che hai notato su questa roadmap. C'è veramente un sacco di roba interessante, ovviamente il tempo per spulciarsi tutto non ce l'ha nessuno di noi, vi segnalo che in realtà la roadmap io l'ho trovata a partire da un progetto pubblico di GitHub che è curato da indiani, giusto per far capire di che livello di competenza parliamo, e lì magari c'è anche qualcosina in più, io ammetto non è che me lo so spulciato più di tanto.

però c'è tanta tanta roba e eventualmente si può contribuire se qualcuno ha anche questi desideri. Però sì, io ho apprezzato molto lo stile con cui viene portata la roba. Prevalentemente non è tanto il modo giusto di... cioè, sicuramente va benissimo per imparare, ma più che altro per avere un feedback sul proprio livello di conoscenze, secondo me. Per esempio, io pensavo di capire qualcosina di Prompt Engineering, ho guardato quella roba e ho detto, ok, devo studiare ancora parecchio, questo per i vari temi. Ok, perfetto, grazie caro, grazie a te. E... nulla ragazzi, va bene ragazzuoli, quello di cui dovevo parlare è finito. Ora interrompo la registrazione.